=== Page 1 ===


=== Page 2 ===
Abstract⼤型语⾔模型（LLMs）在代码⽣成⽅⾯展现了前所未有的能⼒。但⽬前对 LLM ⽣成的代码错误的具体特征尚缺乏深⼊理解。为此，我们针对 HumanEval 数据集上的六种代表性 LLM 进⾏了深⼊分析。⾸先，我们通过开放式编码与主题分析，提炼出⼀套全⾯的代码⽣成错误分类体系；并从语义层⾯和句法层⾯两维度，对错误特征进⾏了刻画。分析表明，LLM 往往会在不同位置产⽣⾮平凡的、多⾏的错误，且根因多样。随后，我们研究了这些错误与任务复杂度及测试通过率之间的关联。研究结果揭示了定位与修复 LLM ⽣成错误的多项挑战，并在最后探讨了若⼲后续应对⽅向。

=== Page 3 ===
I. INTRODUCTION⾃动从⾃然语⾔⽣成代码⼀直是多个研究领域⻓期关注的课题。⼤型语⾔模型（LLMs）的最新进展在此任务上取得了前所未有的快速提升。然⽽，尽管取得了巨⼤进步，LLMs 在许多任务上仍⽆法可靠地⽣成正确代码。⽬前，对 LLM 失败情况的深层次理解仍然不⾜。具体来说，我们尚不清楚 LLM 典型会产⽣哪些类型的代码⽣成错误，也不清楚不同的 LLM 是否会产⽣相似的错误。回答这些问题有助于研究⼈员洞察现有模型的局限，并为模型改进指明⽅向。为弥补这⼀认知空⽩，我们对 LLM ⽣成的代码错误进⾏了深⼊分析。我们选取了六种流⾏的 LLM：CodeGen-16B、InCoder-1.3B、GPT-3.5、GPT-4、SantaCoder 和 StarCoder。在 HumanEval 数据集的 164 道题⽬上，这些模型共⽣成了 557 个错误样本。四位作者协同定位并⼿动修复了这些错误样本的出错部分。针对某些题⽬，LLM 可能提出与 HumanEval 基准解不同的替代⽅案（例如，⽤ lambda 表达式处理数据序列⽽⾮使⽤循环）。为了避免过度拟合ground truth，作者们⼿动定位并修正了错误，遵循LLM的问题解决⽅向。

=== Page 4 ===
I. INTRODUCTION我们进⾏了多轮开放式编码与迭代精炼，以分析所定位错误的特征。具体⽽⾔，我们从两个维度对错误特征进⾏刻画：语义特征 ( Semantic characteristics ) 有助于识别代码⽣成错误的⾼层次根因，代表性特征包括缺失条件、逻辑⽅向错误、条件错误等；句法特征 ( Syntactic characteristics ) 有助于定位错误发⽣的位置，代表性特征包括代码块错误、函数参数错误等。我们的分析表明，⼤多数代码⽣成错误涉及多⾏代码，⽽⾮简单错误。这些错误通常需要对代码进⾏⼤规模重构和修复，⽽⾮简单的局部修改。此外，虽然不同 LLM 在这些错误的句法特征分布（即错误位置）上总体相似，但它们的语义特征（即根本原因）即便在相同任务上也有显著差异。有趣的是，⼤部分错误代码都能顺利编译并运⾏，不会产⽣编译时错误。因此，仅靠编译器检查难以捕捉这些错误，需要通过细致的代码审查和⾼质量的测试⽤例来发现。这也意味着，现代 LLM 已经很好地学习了编程语⾔的句法规则，但在理解⾃然语⾔任务描述的细微差别以及⽣成具备复杂逻辑的精巧代码⽅⾯仍存在不⾜。

=== Page 5 ===
I. INTRODUCTION总结来说，本⽂的主要贡献包括：1.构建了⼀套覆盖语义与句法维度的代码⽣成错误分类体系，采⽤开放式编码与主题分析⽅法；分类标签及标注结果已在 GitHub 公开。2.对⽐分析了不同 LLM 产⽣错误的相似性与差异性，评估了修复这些错误所需的代码编辑代价，同时研究了任务复杂度和测试通过率与各类错误之间的关联。3.探讨了研究结果对改进代码⽣成模型的启示及未来研究⽅向。4.开发了⼀个交互式数据分析⽹站，供研究者和开发者在线查看、筛选与探索各类代码⽣成错误，地址为：https://llm-code-errors.cs.purdue.edu.

=== Page 6 ===
II. METHODOLOGYA.Research QuestionsRQ1： 不同的 LLM 会产⽣哪些类型的代码⽣成错误？旨在发现各模型常⻅错误及其异同，评估是否能开发通⽤改进⽅法或需对模型进⾏专⻔优化。RQ2： 修复这些错误需要投⼊多⼤代价？实际中很难指望 LLM ⽣成完全正确的代码，已有研究表明部分错误代码仍可为开发者提供有⽤起点，因此必须量化修复这些错误所需的⼯作量，并探讨其⾃动化可⾏性。RQ3： 任务复杂度如何影响模型的⽣成表现？直观上，更复杂的任务更具挑战性，但尚不清楚不同 LLM 在⾯对不同复杂度任务时的能⼒边界；若能确定上限，可⽤于指导后续的代码审查、测试与修复⼯作量评估。RQ4： 部分失败（仅未通过部分测试）与完全失败（未通过所有测试）的代码有何特征差异？此问题可揭示达到完全正确所⾯临的特殊挑战。

=== Page 7 ===
II. METHODOLOGYB. Code Generation LLMs选取六种典型 LLM：CodeGen-16B：基于 217 GB BigPython Python 代码训练；InCoder-1.3B：基于 159 GB 来⾃ GitHub、GitLab、StackOverflow 的开源仓库训练；GPT-3.5、GPT-4：OpenAI 提供的通⽤⼤型模型；SantaCoder 与 StarCoder：基于 The Stack 数据集训练。


=== Page 8 ===
II. METHODOLOGYC. Data Collection and Incorrect Solutions在本研究中，我们使⽤⼴泛采⽤的 HumanEval 基准来收集 LLM ⽣成代码时的错误。HumanEval 包含 164 道⼿写的 Python 编程题，每道题平均配备 7.7 个单元测试。这些题⽬涉及语⾔理解、推理、算法和简单数学。对于每道题，我们遵循代码 LLM 性能评测的常⻅做法，使⽤ HumanEval 的原始提示作为输⼊，该提示包括任务描述和平均 2.7 个示例测试⽤例。虽然存在更⾼级的提示策略来增强 LLM 的代码⽣成能⼒，但我们更关⼼其先天能⼒，作为理解其局限的第⼀步。我们在第 VI 节将此视为⼀种有效性威胁进⾏讨论。我们还j将 T 设为 0 以保证结果可复现。随后，我们运⾏测试⽤例以筛选错误解答；并额外进⾏⼀轮⼈⼯复核，以发现那些虽然通过测试但仍不完全正确的解答（因为某些题⽬测试⽤例不充分），共发现 19 个此类情况。示例 1 展示了 GPT-3.5 ⽣成的⼀个错误解答，它未能处理 x 等于 0 的情形——此时输出应为“0”⽽⾮空字符串，但该测试⽤例在 HumanEval 基准中并未包含。最终，我们共确认了六款模型⽣成的 557 个错误解答。


=== Page 9 ===
II. METHODOLOGYD. Manual Analysis of Incorrect Code Solutions对 557 个错误代码解答进⾏了开放式编码，并构建了 LLM 代码⽣成错误的分类体系。开放式编码：从 557 个错误解答中随机抽取 160 个样本，具有 90% 置信度和 5.5% 误差范围。两位作者独⽴定位每个错误解答的错误位置，并记录根本原因；对于多处错误的解答，则对每个错误分别标注特征。由于 LLM 可能提出与 HumanEval 基准不同的替代⽅案，我们选择⼿动调试错误解答（step-by-step debugging）——执⾏失败的测试⽤例——在代码中逐⾏打断点或插⼊打印——找出导致失败的那⾏／那段逻辑——记录“出错的根本原因”（⽐如“漏掉了某个边界条件”或“循环的⽅向写反了”）。初次编码后，两位作者把所有标注下来的“错误位置”（哪⾥错）和“根本原因”（为什么错）整理成表格，然后与其他作者⼀起讨论。归纳出两⼤类特征：语义特征、句法特征。构建初版“错误特征⼿册”（Codebook），包含语义特征 7 类，句法特征 8 类

=== Page 10 ===
II. METHODOLOGYD. Manual Analysis of Incorrect Code Solutions迭代精炼编码⼿册：初版完成后，⼜邀请了另外两位作者加⼊，共四⼈参与。四位作者⾸先独⽴分析了10个错误的代码⽚段，遵循上述描述的相同程序，并根据初始代码簿对错误特征进⾏标记。如果识别出新的特征，⼀位作者会创建⼀个新的标签来描述该特征。第⼀轮标注结束后，四⼈⼀起算出了 Fleiss’ κ （ Fleiss’ Kappa ），⽤来衡量“不同标注者对同⼀份样本的标注⼀致度”。初始 κ 值分别只有 0.37（语义） 和 0.32（句法），都属于“⼀致性偏低”级别。作者们讨论后发现：⼀致性低的主要原因，是“初版⼿册⾥遗漏了太多错误类型”，导致标注时常常找不到合适标签。

=== Page 11 ===
II. METHODOLOGYD. Manual Analysis of Incorrect Code Solutions基于讨论，他们把标签总数从 7+8 扩充到 11 类语义 和 13 类句法，然后⼜做了第⼆轮 10 个样本的标注。这⼀轮的 Fleiss’ κ 跃升⾄ 0.68（语义） 和 0.69（句法），已经进⼊“实质性⼀致”区间。再次讨论并微调标签后，扩充到 13+14，进⾏第三轮 10 个新样本标注。此时 κ 分别达 0.84 和 0.71，且“⽆新增错误类型”——说明标签基本完备。分析剩余数据集：最后，两位作者⽤这套 13 个语义特征 × 14 个句法特征 的“终极⼿册”，标注了其余的错误样本。全量标注后得到 κ = 0.91/0.91（语义/句法），属于“⾼度⼀致”⽔平。尽管如此，还是有 29 个语义标签和 28 个句法标签在最初标注时出现分歧。于是，所有作者再次聚在⼀起，针对这 57 处有争议的标注展开讨论——确认正确类别、消除歧义。最终，讨论中没有发现任何新的错误特征，⼀切分歧都得到了统⼀。⾄此，⼀个覆盖 LLM 常⻅错误的、既能定位⼜能归因的“双维度”标签体系就此敲定，整个过程耗时约 328 ⼈时。

=== Page 12 ===
II. METHODOLOGYE. Analysis of Repair Effort⽬标是回答 RQ2，设计了三种不同的相似度/编辑距离度量，⽤以量化“错误代码”到“正确代码”之间需要的修改量。在计算任意⽂本相似度或编辑距离前，⾸先剔除 LLM ⽣成的所有注释。这样做是因为注释字数和内容并不直接反映代码正确性，若保留注释会⼈为抬⾼两段代码的相似度或掩盖真实的修改代价。Levenshtein 距离：衡量将错误代码变为正确代码，⾄少需要的 插⼊、删除、替换 操作次数。


=== Page 13 ===
II. METHODOLOGYE. Analysis of Repair EffortJaccard 相似度：计算两段代码的 交集⼤⼩ / 并集⼤⼩，Jaccard 值越⾼，说明两段代码在内容上重合越多，对应的修改代价相对越低；反之，则相似度低、修改量⼤。CodeBERTScore：语义相似度，将错误代码和正确代码分别输⼊ CodeBERT，⽤同⼀层的隐藏向量提取每个 token 的 embedding，计算余弦相似度。J=|Serror∩Scorrect||Serror∪Scorrect|

=== Page 14 ===
III. RESULTSA. RQ1: Characteristics of Code Generation Errors表II和表III展示了LLM⽣成的代码错误的最终分类法。该分类法根据其语义特征（即根本原因）和句法特征（即错误位置）对代码⽣成错误进⾏分类。总共有7个类别中的13个语义特征和7个类别中的14个句法特征。1) Semantic Characteristics:Condition Error（条件错误）：Missing Condition（缺失条件）：算法本应在某个分⽀或循环中检查⼀个必要条件，却完全漏写了该判断；Incorrect Condition（错误条件）：在 if 语句或循环的判断表达式中，条件本身写错了（⽐如把 < 写成 <=，或把 and 写成了 or），导致逻辑跑偏。Constant Value Error（常量值错误）：在赋值、函数参数、默认参数或其它地⽅给出了不正确的常量值。例如，应该⽤ math.pi，却硬写成 3.14；或者本应返回 0，却返回了 ""（空字符串）。Reference Error（引⽤错误）：Wrong Method/Variable（错误⽅法或变量）：调⽤了不存在或不适⽤的函数/变量名；Undefined Name（未定义名称）：引⽤了⼀个根本没定义过的标识符。

=== Page 15 ===
III. RESULTSOperation/Calculation Error（运算/计算错误） 在数学或逻辑运算中使⽤了错误的操作符或表达式结构。例如，把⼩于等于写成了⼩于，将加号写成了乘号，或者在返回语句⾥⽤了不恰当的⽐较。Garbage Code（垃圾代码）与核⼼任务⽆关、冗余或完全⽆意义的代码⽚段：Meaningless Snippet（⽆意义代码⽚段）：虽然语法正确，但与题意完全不符；Only Comments（纯注释）：⽣成的代码只有注释，没有任何可执⾏语句；Wrong Logical Direction（逻辑⽅向错误）：整体思路偏离题意，例如把求最⼤值的任务写成了统计总和。Incomplete Code / Missing Steps（不完整代码/缺失步骤）：缺少完成任务所必须的关键步骤，例如构造结果列表却忘了返回它，或者在多步处理流程中漏掉了中间环节。Memory Error（内存/循环错误）：Infinite Loop（⽆限循环）：循环或递归写法导致⽆法终⽌。

=== Page 16 ===


=== Page 17 ===
III. RESULTS共同：「错误条件（Incorrect Condition）」和「逻辑⽅向错误（Wrong Logical Direction）」较⼩的模型（InCoder-1.3B、CodeGen-16B）更容易产⽣“垃圾代码（Garbage Code）”和“缺失多步（Missing Steps）”类型的错误，⽽更⼤的模型倾向于犯“常量数值写错（Constant Value Error）”或“算术运算错误（Operation/Calculation Error）”这样更细节层⾯的失误。所有LLM在理解复杂任务要求和⽣成正确的逻辑条件⽅⾯都存在困难。与GPT-3.5等超⼤型模型相⽐，⼩型模型⽣成更多⽆意义的代码和缺少多个步骤的代码。


=== Page 18 ===
III. RESULTS2) Syntactic Characteristics:Conditional Error（条件错误）：指错误发⽣在 if 语句内部，导致程序⾏为不正确。Loop Error（循环错误）：指在 for 或 while 循环中出现迭代问题，循环边界设置不当，循环变量管理错误。Return Error（返回错误）：return 语句本身有误，返回了错误的值或不符合期望格式的结果。Method Call Error（⽅法调⽤错误）：函数调⽤处出错，可能是函数名写错、参数数量/顺序/类型不对，或者把⽅法调⽤在了错误的对象上。Assignment Error（赋值错误）：赋值语句内出现问题进⽽让后续逻辑建⽴在错误状态上。Import Error（导⼊错误）：导⼊语句中的错误，例如导⼊了不存在的模块、漏导必要模块，导致运⾏阶段出错。Code Block Error（代码块错误）：⼀整段连续语句被错误⽣成或直接遗漏，这类错误往往不是单⾏修补能解决的，需要对整段逻辑重写或补⻬，修复成本⾼。

=== Page 19 ===


=== Page 20 ===
III. RESULTS
与语义特征那部分类似。对所有模型来说，错误最常集中在完整代码块（多条连续语句）和 if 语句这两类结构上。这说明很多错误并⾮“⼩瑕疵”，⽽是⼤块逻辑⽣成失败，需要较⼤代价修复（与 RQ2 的结论呼应）。相⽐其他模型，GPT-4 的错误类型更集中，其他模型的错误位置更加分散、类型更多。进⼀步，CodeGen-16B 和 InCoder-1.3B 更常把函数名写错；GPT-3.5、SantaCoder、StarCoder 则更常犯参数错误。这暗示 CodeGen 和 InCoder 在预训练阶段，对“任务描述 → 应该调⽤哪个函数”的映射掌握得不牢。当前 LLM 在⽣成⼤段结构化代码和关键控制结构时薄弱，修复这些错误需要较⾼成本。

=== Page 21 ===
III. RESULTS3) Mappings Between Semantic and Syntactic Characteristics:
某些语义特征经常与特定的句法特征成对出现。例如，“逻辑⽅向错误”通常对应“代码块错误”。 单个句法特征可能关联多种语义根因

=== Page 22 ===
III. RESULTS3) Mappings Between Semantic and Syntactic Characteristics:
即使错误出现在相似的位置，其语义根因也可能千差万别，因此需要采⽤不同的修复⽅式。——定位只是第⼀步，真正修好还得看根因。
⼀种语义特征，如常量值错误，可能出现在不同的位置。精确地定位此类错误可能具有挑战性。

=== Page 23 ===
III. RESULTS4) The Impact of Training Data:在语义层⾯，GPT-3.5 和 GPT-4 没有产⽣任何“⽆意义代码”⽚段，⽽另外四个模型都会出现。这很可能源于前两者训练数据规模巨⼤，使它们更有能⼒避开完全跑题或毫⽆作⽤的输出。在句法层⾯，作者发现 CodeGen-16B ⽣成“错误代码块”的⽐例反⽽更⼩。这可能与它只⽤ Python 代码进⾏专⻔化训练有关：单⼀语⾔的深度学习，有助于减少⼤段语法结构性错误。⽽其他模型是在多语⾔代码语料上训练的，可能导致在具体语⾔（如 Python）上⽣成整段结构时更容易失⼿。

=== Page 24 ===
III. RESULTSB. RQ2: Repair Effort for Code Generation ErrorsLevenshtein：所有模型的编辑距离分布都很宽，中位数⼤约或⾼于 100。更具体地说，84.21% 的错误代码需要 超过 50 次编辑，52.63% 甚⾄要 200 次以上。这说明⼤多数错误并不是改⼏处⼩地⽅就能搞定的。CodeBERTScore：除 GPT-3.5 的中位数只有 0.05 外，其余六个模型的中位数都在 0.2 左右。⽽且，各模型中有⼤量样本的 CodeBERTScore 低于 0.1.


=== Page 25 ===
III. RESULTS总体来看，⽂本相似度低 + 语义相似度低，与 RQ1 的发现⼀致：LLM 常犯的不是“⼩错”，⽽是“缺失多步”“逻辑⽅向错误”这类⾮平凡问题，修起来需要⼤量编辑。例如，“缺失多步”这类错误的中位编辑次数就达 108 次。有趣的是，GPT-3.5 和 GPT-4 虽然整体性能最好，但⼀旦出错，偏差反⽽更⼤：它们的 Levenshtein 距离中位数⽐其他模型更⾼。这意味着它们通常要么对，要么⼀错就错成“⼤段代码块错误”，修复所需⼯作量反⽽更多。

=== Page 26 ===
III. RESULTS作者借鉴⾃动程序修复（APR）领域的做法，把所有错误按修复粒度分成三类。GPT-3.5 与 GPT-4 的三类错误分布更均衡；SantaCoder 的单⾏错误最多（35%）；StarCoder 的单块错误最多（57%）；InCoder-1.3B（准确率最低的模型）多块错误最多（41%），意味着它的错误往往散落在多处，修起来最麻烦。


=== Page 27 ===
III. RESULTSC. RQ3: Impact of Task Complexity使⽤任务描述的⻓度（即提示中的词数）和正确解决⽅案的⾏数（LOC）作为任务复杂性的代理指标。单纯⽤ LOC 衡量复杂度会失真：有些题（论⽂⾥说有 20 道）标准答案只有⼀⾏，但那⼀⾏可能是⼀个嵌套很深的 lambda 或列表推导式，看起来“1 ⾏”，实则语法结构很复杂。⽐较“成功完成的任务”和“失败任务”的复杂度是否显著不同：
对所有 6 个模型⽽⾔，“通过”的任务与“失败”的任务在两项复杂度代理指标上都存在显著统计差异（作者单独做了p 值检验）失败的任务通常提示更⻓（描述⽂字更多、更复杂），标准解答⾏数更多，语法结构也更复杂（AST 节点更多）。

=== Page 28 ===
III. RESULTS进⼀步深⼊查看每个模型中“提示⻓度超过 150 个词”的失败任务。按之前的语义分类，这些失败⾥有 64.0% 属于“垃圾代码”——即与任务⽬标⽆关的⽆效代码。举例任务 129 的提示⻓达 249 词，要求在给定⽹格中找到⻓度为 k、路径和最⼩的路径。InCoder-1.3B 没理解复杂需求，只⽣成了⼀串毫⽆意义的 append 操作。在 LOC 维度上也出现类似模式：当标准答案超过 12 ⾏代码时，失败的任务中有 55.9% 属于垃圾代码。⽐如任务 105 需要三步：排序（1–9）、反转、把每个数字换成对应的英⽂字符串。正确答案有 24 ⾏代码，但 CodeGen-16B 没弄懂要求，只返回了⼀个空数组。


=== Page 29 ===
III. RESULTSD. RQ4: Impact of Test Pass Rate把没通过所有测试的代码细分为完全失败（所有⽤例都挂）和部分失败（只挂⼀部分⽤例），并排除了那 19 个“全通过但其实不等价”的特殊案例。⾸先看语义特征。纯注释（only comments）和⽆意义代码（meaningless code）是最容易导致完全失败的两类根因。看似⼩问题的“未定义名称”（undefined name）也常常导致完全失败，因为⼀运⾏就异常崩溃。反⽽像“缺失多步”这种听起来很严重的错误，经常只是让模型漏掉部分需求，因此还能侥幸通过⼀些测试——说明测试⽤例不够强，或者模型并⾮完全误解题意。


=== Page 30 ===
III. RESULTS举例：Task 125 需要在“字符串没有空格”的情况下按逗号分割；若也没有逗号，还要继续做别的处理。CodeGen-16B 只实现了按空格切分，后⾯步骤全漏，于是它只能通过包含空格的测试，⽤例⾥没空格时就挂了，属于“部分失败”。还有⼀个意外发现是“逻辑⽅向错误”的代码也可能偶然通过⼏例测试。⽐如 Task 75，InCoder-1.3B 没按题意实现，但却能过 5、10、30 这些测试。合理推测是：提示⾥⾃带的测试样例给了模型“表层关联”，它记住了某些输⼊-输出组合，从⽽⽤不相关的逻辑碰巧命中部分⽤例。


=== Page 31 ===
III. RESULTS再看句法特征。如果错误发⽣在 if 语句（if error），82% 是部分失败，18% 才是完全失败。原因是 if 错误往往只是误解了其中某个条件，其余代码可能仍然正确，所以能过掉⼀些⽤例。
⽐如 Task 0，SantaCoder 只考虑了列表中相邻元素，忽略了⾮相邻的情况，因此错过了那条特定测试，形成部分失败。


=== Page 32 ===
IV. DISCUSSION AND FUTURE WORK1. 如何修复 LLM ⽣成的错误代码： 作者指出：不同模型犯错的语义与句法特征差异巨⼤（Finding 1、2）。像 if 条件错误、函数参数错误这种单⾏或单点逻辑问题，传统⾃动程序修复（APR）技术已经⽐较擅⻓修复。但论⽂的发现也显示，⼤量错误其实是多⾏、多块的结构性问题。为突破这⼀局限，近来有⼈⽤ LLM 本身来做“程序修复代理”（Program Repair Agents），依靠错误信息等粗粒度信号引导修复。本⽂贡献的细粒度分类（语义+句法）可以更精确地指导修复：先训练⼀个模型预测错误类型，再把预测结果嵌⼊提示词⾥引导修复代理。例如，不再是笼统地说“修 Line 6 的 bug”，⽽是明确提示“修正第 6 ⾏的函数参数错误”。

=== Page 33 ===
IV. DISCUSSION AND FUTURE WORK⽤ GPT-4 做了⼀个⼩实验验证：在 HumanEval 的 18 个错误⾥，如果加⼊他们的标签信息，GPT-4 能修好 7 个（不⽤标签时只修好了 4 个）；在更难的 BigCodeBench-Hard 上，带标签能修好 12 个，⽽不⽤标签只能修好 3 个。说明这套分类越在困难任务上越能发挥作⽤。2. 如何更精准地定位错误：找准“错在哪⼉”是修复前的第⼀步，但 Finding 2 显示错误可能出现在各种结构⾥，定位并不容易。传统定位⽅法依赖⾼质量、覆盖全⾯的测试⽤例（谱系法等），构造这些⽤例很费⼒；深度学习式定位通常靠源代码特征和错误信息。但 LLM ⽣成代码还有更多可利⽤的信号：如每步解码的 logits、token 概率分布、⾃注意⼒分数等。未来可以探索把这些“⽣成过程信号”与错误消息、代码特征⼀起⽤来预测模型从哪⼀步开始跑偏。

=== Page 34 ===
IV. DISCUSSION AND FUTURE WORK3. 如何估计代码正确性与任务上限：Finding 4 表明，当前 LLM 对复杂任务仍很吃⼒。这⾥有两类问题：1）能否给定⼀个模型，估计它能处理的任务复杂度上限？论⽂⾥只⽤了提示⻓度、LOC 这类简单指标，后续可以考虑更精细的度量，以便准确评估模型能⼒边界，提⾼⽣成可靠性。2）能否仅凭提示特征（如⻓度）来预测任务难度和最终代码是否正确？已有⼯作尝试让 LLM 直接当“评审员”，不依赖测试⽤例就判定代码是否正确（如 CodeJudge 在 HumanEval 上做到 73.13%，但在 BigCodeBench 上只有 54.56%）。这类技术若更成熟，可帮助开发者根据“正确性估计”分配审查与测试资源。

=== Page 35 ===
Threats to ValidityThreats to Internal Validity：最⼤的⻛险来⾃⼈⼯标注：不同标注者可能理解不⼀、甚⾄出错。为降低偏差，作者先由四名作者共同进⾏开放式编码并多轮迭代完善代码本（codebook），在达成“较⾼⼀致性”后，才由其中两⼈给完整数据集打标签。最终语义与句法两类特征的 Fleiss’ κ 都达到 0.91，属于“⼏乎完美⼀致”，说明标注可靠。Threats to External Validity：数据集局限：由于标注成本极⾼（557 段错误代码共耗时约 328 ⼈时，需要运⾏代码并逐步调试定位根因），本研究只标注了⼀个数据集（HumanEval）。任务类型局限：研究仅涉及函数级的代码⽣成错误；尚未覆盖类级别⽣成、代码摘要、重构等其它任务类型。语⾔局限：只研究了 Python 代码。Python 与 PHP、Rust 等差异较⼤，结论未必适⽤。模型与提示策略局限：只分析了 6 个 LLM，且只⽤了⼀种提示策略（原始提示 + 温度 0）。

=== Page 36 ===
CONCLUSION本⽂对 LLM 在代码⽣成中出现的错误进⾏了实证研究。研究团队基于 HumanEval 数据集上六个主流模型的失败案例，通过开放式编码与主题分析，构建了⼀套覆盖语义与句法两个维度的错误分类体系，并据此为 557 个错误实例贴标签。结果显示，不同模型在语义错误和句法错误的分布上各有差异。作者进⼀步分析了修复这些错误所需的代价、任务复杂度对结果的影响、以及测试通过率与不同错误类型之间的关系。最后，论⽂讨论了这些发现对改进代码⽣成 LLM 质量与可靠性的启示，并提出了未来研究⽅向。

